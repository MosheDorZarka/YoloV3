{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MosheDorZarka/YoloV3/blob/main/YOLOv3_SelfAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dznHe15KGaLP"
      },
      "source": [
        "\n",
        "### In this project, I aim to enhance the YoloV3 model by integrating a novel Self-Attention (SA) mechanism. My primary objective is to investigate whether this augmented YoloV3 model outperforms its traditional counterparts in terms of accuracy and efficiency. This endeavor represents an exploration in the field of object detection, leveraging advanced techniques to potentially enhance the performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PxVM24pHGErQ"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade keras-cv\n",
        "!pip install -q --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8D4hotlItwB"
      },
      "outputs": [],
      "source": [
        "!pip show keras\n",
        "!pip show keras-cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XTc-7GdSIh9S"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_cv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from keras.layers import (\n",
        "    Layer,\n",
        "    Conv2D,\n",
        "    Embedding,\n",
        "    BatchNormalization,\n",
        "    LeakyReLU,\n",
        "    MultiHeadAttention,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    LayerNormalization,\n",
        "    Input\n",
        ")\n",
        "from keras.ops import (\n",
        "    expand_dims,\n",
        "    arange,\n",
        "    reshape,\n",
        "    shape,\n",
        "    scatter_update,\n",
        "    cast,\n",
        "    minimum,\n",
        "    argmax,\n",
        "    stack,\n",
        "    concatenate,\n",
        "    zeros,\n",
        "    ones_like,\n",
        "    zeros_like,\n",
        "    tile,\n",
        "    shape,\n",
        "    sigmoid,\n",
        "    exp,\n",
        "    log,\n",
        "    max,\n",
        "    where,\n",
        "    square,\n",
        "    sqrt,\n",
        "    sum,\n",
        "    average,\n",
        "    binary_crossentropy,\n",
        "    sparse_categorical_crossentropy,\n",
        "    meshgrid,\n",
        "    equal,\n",
        "    one_hot,\n",
        "    softmax\n",
        ")\n",
        "from keras_cv.bounding_box import (\n",
        "    compute_iou,\n",
        "    convert_format,\n",
        "    to_dense\n",
        ")\n",
        "from keras_cv.visualization import (\n",
        "    plot_bounding_box_gallery\n",
        ")\n",
        "from keras_cv.layers import (\n",
        "    RandomFlip,\n",
        "    JitteredResize,\n",
        "    MultiClassNonMaxSuppression,\n",
        "    Resizing\n",
        ")\n",
        "from keras.regularizers import l2\n",
        "from keras.activations import gelu\n",
        "from keras.applications import ResNet50\n",
        "from keras.applications.resnet import preprocess_input\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import Model\n",
        "from tensorflow_datasets import load\n",
        "from tensorflow import data as tf_data\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOwkvt6P44QY"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NCqL_RGG4--p"
      },
      "outputs": [],
      "source": [
        "ANCHORS = [\n",
        "    [[116, 90], [156, 198], [373, 326]],\n",
        "    [[30, 61], [62, 45], [59, 119]],\n",
        "    [[10, 13], [16, 30], [33, 23]]\n",
        "    ]\n",
        "IMAGE_DIMENSION = 416\n",
        "GRID_SIZES = [13, 26, 52]\n",
        "NUM_CLASSES = 20\n",
        "CLASSES = [\n",
        "    \"Aeroplane\",\n",
        "    \"Bicycle\",\n",
        "    \"Bird\",\n",
        "    \"Boat\",\n",
        "    \"Bottle\",\n",
        "    \"Bus\",\n",
        "    \"Car\",\n",
        "    \"Cat\",\n",
        "    \"Chair\",\n",
        "    \"Cow\",\n",
        "    \"Dining Table\",\n",
        "    \"Dog\",\n",
        "    \"Horse\",\n",
        "    \"Motorbike\",\n",
        "    \"Person\",\n",
        "    \"Potted Plant\",\n",
        "    \"Sheep\",\n",
        "    \"Sofa\",\n",
        "    \"Train\",\n",
        "    \"Tvmonitor\",\n",
        "    \"Total\",\n",
        "]\n",
        "BATCH_SIZE = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwIYoics46G9"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PpIe4vKP4_bU"
      },
      "outputs": [],
      "source": [
        "def bbox_to_output_loop(classes, boxes):\n",
        "  \"\"\"\n",
        "  Transforming a bbox with rel_center_xywh format to a valid output suitable for loss function.\n",
        "\n",
        "  Args:\n",
        "  - classes: A tensor of shape (N, )\n",
        "  - boxes: A tensor of shape (N, 4)\n",
        "\n",
        "  Returns:\n",
        "  - scale_1, scale,2, scale_3: A tuple of tensors in the shape of (scale_grid_size, scale_grid_size, 3, 4 + 1 + 1)\n",
        "  \"\"\"\n",
        "  outputs = []\n",
        "\n",
        "  for scale, anchors in enumerate(ANCHORS):\n",
        "    grid_x = cast(boxes[..., 0] * GRID_SIZES[scale], dtype=\"int32\") # shape: (N, )\n",
        "    grid_y = cast(boxes[..., 1] * GRID_SIZES[scale], dtype=\"int32\") # shape: (N, )\n",
        "\n",
        "    anchors_norm_np = np.array(anchors, dtype=\"float32\") / IMAGE_DIMENSION # shape: (3, 2)\n",
        "\n",
        "    # Calculating IoU's to find the best anchors\n",
        "    bbox_w, bbox_h = boxes[..., 2:3], boxes[..., 3:4] # shape: (N, 1)\n",
        "    anchors_norm_np_w, anchors_norm_np_h = expand_dims(anchors_norm_np[..., 0], axis=0), \\\n",
        "                                           expand_dims(anchors_norm_np[..., 1], axis=0)  # shape: (1, 3)\n",
        "    intersection = minimum(bbox_w, anchors_norm_np_w) * minimum(bbox_h, anchors_norm_np_h) # shape: (N, 3)\n",
        "    union = (bbox_w * bbox_h + anchors_norm_np_w * anchors_norm_np_h) - intersection # shape: (N, 3)\n",
        "    IoUs = intersection / union\n",
        "\n",
        "    best_anchors = argmax(IoUs, axis=-1) # shape: (N, )\n",
        "\n",
        "    # converting xy to be relative to cells\n",
        "    grid_xy = stack([grid_x, grid_y], axis=-1)\n",
        "    bbox_xy = boxes[..., 0:2] * GRID_SIZES[scale] - cast(grid_xy, dtype=\"float32\") # shape: (N, 2)\n",
        "    bbox_rel = concatenate([bbox_xy, boxes[..., 2:4]], axis=-1) # shape: (N, 4)\n",
        "\n",
        "    indices = stack([grid_y, grid_x, best_anchors], axis=-1) # shape: (N, 3)\n",
        "    classes_expanded = expand_dims(classes, axis=-1)\n",
        "    updates = concatenate([bbox_rel, ones_like(classes_expanded), classes_expanded], axis=-1) # shape (N, 4 + 1 + 1)\n",
        "\n",
        "    scale_1 = scatter_update(inputs=zeros(shape=(GRID_SIZES[scale], GRID_SIZES[scale], 3, 4 + 1 + 1)), indices=indices, updates=updates)\n",
        "\n",
        "    outputs += [scale_1]\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vNWoQscIwMOg"
      },
      "outputs": [],
      "source": [
        "def bbox_to_output_vec(classes, boxes):\n",
        "  \"\"\"\n",
        "  Transforming a bbox with rel_center_xywh format to a valid output suitable for loss function.\n",
        "\n",
        "  Args:\n",
        "  - classes: A tensor of shape (N, )\n",
        "  - boxes: A tensor of shape (N, 4)\n",
        "\n",
        "  Returns:\n",
        "  - scale_1, scale,2, scale_3: A tuple of tensors in the shape of (scale_grid_size, scale_grid_size, 3, 4 + 1 + 1)\n",
        "  \"\"\"\n",
        "  grid_sizes_np = np.array([GRID_SIZES], dtype=\"float32\") # shape: (1, 3)\n",
        "\n",
        "  grid_x = cast(boxes[..., 0:1] * grid_sizes_np, dtype=\"int32\") # shape: (N, 3)\n",
        "  grid_y = cast(boxes[..., 1:2] * grid_sizes_np, dtype=\"int32\") # shape: (N, 3)\n",
        "\n",
        "  anchors_norm_np = np.array(ANCHORS, dtype=\"float32\") / IMAGE_DIMENSION # shape: (3, 3, 2)\n",
        "\n",
        "  # Calculating IoU's to find the best anchors\n",
        "  boxes_w, boxes_h = expand_dims(boxes[..., 2:3], axis=1), expand_dims(boxes[..., 3:4], axis=1) # shape: (N, 1, 1)\n",
        "  anchors_norm_np_w, anchors_norm_np_h = expand_dims(anchors_norm_np[..., 0], axis=0), \\\n",
        "                                         expand_dims(anchors_norm_np[..., 1], axis=0)  # shape: (1, 3, 3)\n",
        "  intersection = minimum(boxes_w, anchors_norm_np_w) * minimum(boxes_h, anchors_norm_np_h) # shape: (N, 3, 3)\n",
        "  union = (boxes_w * boxes_h + anchors_norm_np_w * anchors_norm_np_h) - intersection # shape: (N, 3, 3)\n",
        "  IoUs = intersection / union\n",
        "\n",
        "  best_anchors = argmax(IoUs, axis=-1) # shape: (N, 3)\n",
        "\n",
        "  # converting xy to be relative to cells\n",
        "  grid_xy = stack([grid_x, grid_y], axis=-1) # shape: (N, 3, 2)\n",
        "  boxes_xy = expand_dims(boxes[..., 0:2], axis=1) * expand_dims(grid_sizes_np, axis=-1) - cast(grid_xy, dtype=\"float32\") # shape: (N, 3, 2)\n",
        "  boxes = concatenate([boxes_xy, tile(expand_dims(boxes[..., 2:4], axis=1), repeats=(1, 3, 1))], axis=-1) # shape: (N, 3, 4)\n",
        "\n",
        "  indices = stack([grid_y, grid_x, best_anchors], axis=-1) # shape: (N, 3, 3)\n",
        "  classes = tile(reshape(classes, newshape=(-1, 1, 1)), repeats=(1, 3, 1)) # shape: (N, 3, 1)\n",
        "  updates = concatenate([boxes, ones_like(classes), classes], axis=-1) # shape (N, 3, 4 + 1 + 1)\n",
        "\n",
        "  scale_1 = scatter_update(inputs=zeros(shape=(GRID_SIZES[0], GRID_SIZES[0], 3, 4 + 1 + 1)), indices=indices[..., 0, :], updates=updates[..., 0, :])\n",
        "  scale_2 = scatter_update(inputs=zeros(shape=(GRID_SIZES[1], GRID_SIZES[1], 3, 4 + 1 + 1)), indices=indices[..., 1, :], updates=updates[..., 1, :])\n",
        "  scale_3 = scatter_update(inputs=zeros(shape=(GRID_SIZES[2], GRID_SIZES[2], 3, 4 + 1 + 1)), indices=indices[..., 2, :], updates=updates[..., 2, :])\n",
        "\n",
        "  return scale_1, scale_2, scale_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rn4oSDxz6JzZ"
      },
      "outputs": [],
      "source": [
        "def unpackage_raw_sample(sample):\n",
        "  \"\"\"\n",
        "  Unpacking tfds raw samples and transforming it to be in valid format for KerasCV components.\n",
        "  ({\"images\": images,\n",
        "    \"bounding_boxes\": {\"classes\": classes, \"boxes\": boxes}}).\n",
        "  This function assumes the samples bbox are in rel_yxyx format.\n",
        "\n",
        "  Args:\n",
        "  - sample: a raw tfds sample (FeaturesDict).\n",
        "\n",
        "  Returns:\n",
        "  - x: a valid format for KerasCV components.\n",
        "  \"\"\"\n",
        "  # Unpacking the sample\n",
        "  image = sample[\"image\"]\n",
        "  classes = sample[\"objects\"][\"label\"]\n",
        "  boxes = convert_format(\n",
        "      boxes=sample[\"objects\"][\"bbox\"],\n",
        "      images=image,\n",
        "      source=\"rel_yxyx\",\n",
        "      target=\"center_xywh\"\n",
        "  )\n",
        "\n",
        "  # Creating the valid output format\n",
        "  x = {\n",
        "      \"images\": image,\n",
        "      \"bounding_boxes\": {\"classes\": classes, \"boxes\": boxes}\n",
        "      }\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YOkoQyG7BkQ-"
      },
      "outputs": [],
      "source": [
        "def preprocessing_wrapper(augmenters):\n",
        "  def preprocessing(x):\n",
        "    \"\"\"\n",
        "    Tranforming a valid KerasCV format sample to be a proper input and label for model trainig.\n",
        "\n",
        "    Args:\n",
        "    - x: a valid KerasCV format.\n",
        "\n",
        "    Returns:\n",
        "    - image, labels: A tuple of the preprocessed image and (scale_1, scale_2, scale_3) label.\n",
        "    \"\"\"\n",
        "\n",
        "    # apply augmentations\n",
        "    for augmenter in augmenters:\n",
        "      x = augmenter(x)\n",
        "\n",
        "    # apply ResNet50 preprocessing & tranforming the bbox to be valid labels\n",
        "    image = preprocess_input(x[\"images\"])\n",
        "\n",
        "    x[\"bounding_boxes\"] = to_dense(x[\"bounding_boxes\"])\n",
        "    x[\"bounding_boxes\"][\"boxes\"] = x[\"bounding_boxes\"][\"boxes\"] / IMAGE_DIMENSION\n",
        "    labels = bbox_to_output_vec(**x[\"bounding_boxes\"])\n",
        "\n",
        "    return image, labels\n",
        "\n",
        "  return preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_tcNWvL7wDYy"
      },
      "outputs": [],
      "source": [
        "def output_to_bbox(scale, anchors):\n",
        "  \"\"\"\n",
        "  Extracting from a single model output the boxes and class predictions.\n",
        "  This function is particularly helpful before applying NonMaxSuppression layer.\n",
        "\n",
        "  Args:\n",
        "  - scale: A tensor of shape (Batch, grid_scale_x, grid_scale_y, 3 * (4 + 1 + NUM_CLASSES))\n",
        "\n",
        "  Returns:\n",
        "  - box_prediction: A tensor of shape (Batch, N, 4)\n",
        "  - class_prediction: A tensor of shape (Batch, N, NUM_CLASSES)\n",
        "  \"\"\"\n",
        "  batch_size, grid_scale_x, grid_scale_y, _ = shape(scale)\n",
        "  grid_x = arange(0, grid_scale_x, dtype=\"float32\")\n",
        "  grid_y = arange(0, grid_scale_y, dtype=\"float32\")\n",
        "  grid_xs, grid_ys = meshgrid(grid_x, grid_y)\n",
        "  grid_xy = stack([grid_xs, grid_ys], axis=-1) # shape: (Batch, grid_scale_x, grid_scale_y, 2)\n",
        "  grid_xy = expand_dims(grid_xy, axis=-2) # shape: (Batch, grid_scale_x, grid_scale_y, 1, 2)\n",
        "\n",
        "  scale = reshape(scale, newshape=(batch_size, grid_scale_x, grid_scale_y, 3, -1))\n",
        "  scale_xy = sigmoid(scale[..., 0:2])\n",
        "  scale_wh = exp(scale[..., 2:4]) * anchors\n",
        "  scale_confidence = sigmoid(scale[..., 4:5])\n",
        "  scale_cls = softmax(scale[..., 5:])\n",
        "\n",
        "  boxes_xy = (scale_xy + grid_xy) / (grid_scale_x, grid_scale_y)\n",
        "  boxes_wh = scale_wh\n",
        "  boxes_xywh = concatenate([boxes_xy, boxes_wh], axis=-1) * IMAGE_DIMENSION\n",
        "\n",
        "  classes = scale_confidence * scale_cls\n",
        "\n",
        "  box_prediction = reshape(\n",
        "      boxes_xywh,\n",
        "      newshape=(batch_size, grid_scale_x * grid_scale_y * 3, -1)\n",
        "      )\n",
        "  class_prediction = reshape(\n",
        "      classes,\n",
        "      newshape=(batch_size, grid_scale_x * grid_scale_y * 3, -1)\n",
        "      )\n",
        "\n",
        "  return box_prediction, class_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1JUNK5N715fn"
      },
      "outputs": [],
      "source": [
        "def visualize_progress(model, confidence_threshold=0.7, iou_threshold=0.1, skip=10):\n",
        "  \"\"\"\n",
        "  Tracking down by visualization of the prediction of the model the progress\n",
        "  amid training.\n",
        "\n",
        "  Args:\n",
        "  - model: the model being trained\n",
        "\n",
        "  \"\"\"\n",
        "  sample = next(iter(eval_ds.skip(skip).take(1)))\n",
        "  resize_layer = Resizing(IMAGE_DIMENSION, IMAGE_DIMENSION, bounding_box_format=\"center_xywh\", pad_to_aspect_ratio=True)\n",
        "\n",
        "  image = sample[\"image\"]\n",
        "  classes = sample[\"objects\"][\"label\"]\n",
        "  boxes_rel_yxyx = sample[\"objects\"][\"bbox\"]\n",
        "\n",
        "  boxes = convert_format(\n",
        "      boxes=boxes_rel_yxyx,\n",
        "      source=\"rel_yxyx\",\n",
        "      target=\"center_xywh\",\n",
        "      images=image\n",
        "  )\n",
        "\n",
        "  bounding_boxes_true = {\n",
        "      \"classes\": expand_dims(classes, axis=0),\n",
        "      \"boxes\": expand_dims(boxes, axis=0)\n",
        "      }\n",
        "\n",
        "  x = {\n",
        "      \"images\": expand_dims(image, axis=0),\n",
        "      \"bounding_boxes\": bounding_boxes_true\n",
        "      }\n",
        "\n",
        "  sample_resized = resize_layer(x)\n",
        "\n",
        "  input = preprocess_input(sample_resized[\"images\"])\n",
        "  scale_1, scale_2, scale_3 = model(input)\n",
        "\n",
        "  anchors_1 = np.array(ANCHORS[0])/IMAGE_DIMENSION\n",
        "  anchors_2 = np.array(ANCHORS[1])/IMAGE_DIMENSION\n",
        "  anchors_3 = np.array(ANCHORS[2])/IMAGE_DIMENSION\n",
        "\n",
        "  box_prediction_1, class_prediction_1 = output_to_bbox(scale_1, anchors=anchors_1)\n",
        "  box_prediction_2, class_prediction_2 = output_to_bbox(scale_2, anchors=anchors_2)\n",
        "  box_prediction_3, class_prediction_3 = output_to_bbox(scale_3, anchors=anchors_3)\n",
        "\n",
        "  box_prediction = concatenate([box_prediction_1, box_prediction_2, box_prediction_3], axis=1)\n",
        "  class_prediction = concatenate([class_prediction_1, class_prediction_2, class_prediction_3], axis=1)\n",
        "\n",
        "  nms = MultiClassNonMaxSuppression(\n",
        "      bounding_box_format=\"center_xywh\",\n",
        "      from_logits=False,\n",
        "      confidence_threshold=confidence_threshold,\n",
        "      iou_threshold=iou_threshold\n",
        "      )\n",
        "\n",
        "  bounding_boxes_pred = nms(box_prediction, class_prediction)\n",
        "  class_mapping = dict(zip(range(len(CLASSES)), CLASSES))\n",
        "\n",
        "  plot_bounding_box_gallery(\n",
        "      sample_resized[\"images\"],\n",
        "      value_range=(0, 255),\n",
        "      rows=1,\n",
        "      cols=1,\n",
        "      y_true=sample_resized[\"bounding_boxes\"],\n",
        "      scale=5,\n",
        "      bounding_box_format=\"center_xywh\",\n",
        "      font_scale=0.5,\n",
        "      class_mapping=class_mapping\n",
        "  )\n",
        "\n",
        "  plot_bounding_box_gallery(\n",
        "      sample_resized[\"images\"],\n",
        "      value_range=(0, 255),\n",
        "      rows=1,\n",
        "      cols=1,\n",
        "      # y_true=sample_resized[\"bounding_boxes\"],\n",
        "      y_pred=bounding_boxes_pred,\n",
        "      scale=5,\n",
        "      bounding_box_format=\"center_xywh\",\n",
        "      font_scale=0.5,\n",
        "      class_mapping=class_mapping\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO6aVCnm49IC"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "u9Zt5a7okr_n"
      },
      "outputs": [],
      "source": [
        "train_ds, eval_ds = load(name=\"voc/2007\", split=[\"train\", 'validation'], with_info=False, shuffle_files=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-Av1Y6RKAmb0"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(lambda sample: unpackage_raw_sample(sample), num_parallel_calls=tf_data.AUTOTUNE)\n",
        "eval_ds = eval_ds.map(lambda sample: unpackage_raw_sample(sample), num_parallel_calls=tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "P8nRPrx2-5IT"
      },
      "outputs": [],
      "source": [
        "train_ds_augmenters = [\n",
        "    RandomFlip(\n",
        "        mode=\"horizontal\",\n",
        "        bounding_box_format=\"center_xywh\"\n",
        "        ),\n",
        "    JitteredResize(\n",
        "        target_size=(IMAGE_DIMENSION, IMAGE_DIMENSION),\n",
        "        scale_factor=(0.65, 1.25),\n",
        "        bounding_box_format=\"center_xywh\"\n",
        "        )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fq9yZJDV_NlI"
      },
      "outputs": [],
      "source": [
        "eval_ds_augmenters = [Resizing(IMAGE_DIMENSION,\n",
        "                               IMAGE_DIMENSION,\n",
        "                               bounding_box_format=\"center_xywh\",\n",
        "                               pad_to_aspect_ratio=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DbZ-qkOUAbHr"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(preprocessing_wrapper(train_ds_augmenters), num_parallel_calls=tf_data.AUTOTUNE)\n",
        "eval_ds = eval_ds.map(preprocessing_wrapper(eval_ds_augmenters), num_parallel_calls=tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kisMRmDoBbn_"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.shuffle(BATCH_SIZE * 4).batch(BATCH_SIZE).prefetch(tf_data.AUTOTUNE)\n",
        "eval_ds = eval_ds.batch(BATCH_SIZE).prefetch(tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWOqeki6HAz2"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "x3w7fp1Xl6CL"
      },
      "outputs": [],
      "source": [
        "class YoloConvBlock(Layer):\n",
        "  \"\"\"\n",
        "  A standart Yolov3 conv block. Used solely in the output layer.\n",
        "  Essentialy, this is a helper layer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, filters=256, kernel_size=3, num_classes=20, num_anchors=3, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.conv1 = Conv2D(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        kernel_regularizer=l2(1e-5)\n",
        "        )\n",
        "    self.conv2 = Conv2D(\n",
        "        filters=num_anchors*(num_classes + 5),\n",
        "        kernel_size=1,\n",
        "        padding=\"same\",\n",
        "        kernel_regularizer=l2(1e-5)\n",
        "        )\n",
        "    self.bn = BatchNormalization()\n",
        "    self.activation = LeakyReLU(negative_slope=0.1)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.activation(self.bn(self.conv1(x)))\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-UAGRRHAodja"
      },
      "outputs": [],
      "source": [
        "class YoloTransformerEncoderBlock(Layer):\n",
        "  \"\"\"\n",
        "  Integrates a Transformer Encoder into YOLO for capturing long-range dependencies\n",
        "  in object detection. This layer employs multi-head self-attention and\n",
        "  feedforward networks, aiming to enhance detection accuracy by understanding\n",
        "  global context. It's an experimental approach to balance precision and\n",
        "  inference speed in complex image scenes.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, grid_size, num_anchors=3, num_classes=20, num_heads=8, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.positions = arange(start=0, stop=grid_size**2, step=1)\n",
        "    self.position_embedding = Embedding(input_dim=grid_size**2, output_dim=num_anchors*(num_classes + 5))\n",
        "    self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=num_anchors*(num_classes + 5), dropout=0.1)\n",
        "    self.ln1 = LayerNormalization()\n",
        "    self.ln2 = LayerNormalization()\n",
        "    self.dense1 = Dense(units=2*num_anchors*(4 + 1 + num_classes), activation=gelu)\n",
        "    self.dense2 = Dense(units=num_anchors*(4 + 1 + num_classes), activation=gelu)\n",
        "    self.drop1 = Dropout(0.1)\n",
        "    self.drop2 = Dropout(0.1)\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    positions = expand_dims(self.position_embedding(self.positions), axis=0)\n",
        "    x = x + positions\n",
        "\n",
        "    x = self.mha(x, x) + x\n",
        "    x = self.ln1(x)\n",
        "\n",
        "    x1 = self.drop1(self.dense1(x))\n",
        "    x1 = self.drop2(self.dense2(x1))\n",
        "\n",
        "    x = x + x1\n",
        "    x = self.ln2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RDYUit8ichpr"
      },
      "outputs": [],
      "source": [
        "class YoloConvSelfAttention(Layer):\n",
        "  \"\"\"\n",
        "  The final layer applied on each of the scales\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, grid_size, num_classes, num_anchors=3, filters=256, kernel_size=3, num_heads=8, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # model layers\n",
        "    self.conv_block = YoloConvBlock(filters, kernel_size, num_classes, num_anchors)\n",
        "    self.encoder_block =  YoloTransformerEncoderBlock(grid_size, num_anchors, num_classes, num_heads)\n",
        "    self.bn = BatchNormalization()\n",
        "    self.drop = Dropout(0.2)\n",
        "    self.pred = Conv2D(filters=num_anchors*(4 + 1 + num_classes), kernel_size=1)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv_block(x)\n",
        "\n",
        "    b, g_x, g_y, out = shape(x)   # X shape: (batch_size, grid_size, grid_size, num_anchors * (4 + 1 + num_classes))\n",
        "    x1 = reshape(x, [b, g_x * g_y, out])   # Reshaping to (batch_size, grid_size * grid_size, self.output_dim)\n",
        "\n",
        "    x1 = self.encoder_block(x1)\n",
        "    x = reshape(x1, [b, g_x, g_y, out]) + x  # Reshaping to original shape\n",
        "\n",
        "    x = self.drop(self.bn(x))\n",
        "    x = self.pred(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_ejE1qAiKP72"
      },
      "outputs": [],
      "source": [
        "class YoloSelfAttention(Model):\n",
        "  \"\"\"\n",
        "  End-to-end yolov3 model with self attention mechanism. This model uses ResNet50\n",
        "  as it's backbone, with classic IMAGE_DIMENSION X IMAGE_DIMENSION X 3 input shape.\n",
        "  The model outputs three outputs (scales): (batch_size, 13 | 26 | 52, 13 | 26 | 52, 3 * (4 + 1 + NUM_CLASSES))\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_anchors=3, num_classes=NUM_CLASSES, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    base_model = ResNet50(\n",
        "      include_top=False,\n",
        "      weights=\"imagenet\",\n",
        "      input_shape=(IMAGE_DIMENSION, IMAGE_DIMENSION, 3),\n",
        "      )\n",
        "    intermediate_layers_output = [base_model.get_layer(\"conv3_block4_out\").output, base_model.get_layer(\"conv4_block6_out\").output, base_model.get_layer(\"conv5_block3_out\").output]\n",
        "    self.backbone = Model(inputs=base_model.input, outputs=intermediate_layers_output)\n",
        "    self.freeze_backbone()\n",
        "\n",
        "    self.scale1 = YoloConvSelfAttention(grid_size=IMAGE_DIMENSION // 32, num_classes=num_classes, num_anchors=num_anchors)\n",
        "    self.scale2 = YoloConvSelfAttention(grid_size=IMAGE_DIMENSION // 16, num_classes=num_classes, num_anchors=num_anchors)\n",
        "    self.scale3 = YoloConvSelfAttention(grid_size=IMAGE_DIMENSION // 8, num_classes=num_classes, num_anchors=num_anchors)\n",
        "\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    s3, s2, s1 = self.backbone(x, training=training)   # output shape: (N, 52 | 26 | 13, 52 | 26 | 13, 512 | 1024 | 2048)\n",
        "\n",
        "    s1 = self.scale1(s1)\n",
        "    s2 = self.scale2(s2)\n",
        "    s3 = self.scale3(s3)\n",
        "\n",
        "    return s1, s2, s3\n",
        "\n",
        "  def freeze_backbone(self):\n",
        "    \"\"\"\n",
        "    Freeze the backbone trainable parameters. Use when transfer learning.\n",
        "    \"\"\"\n",
        "    self.backbone.trainable = False\n",
        "\n",
        "  def unfreeze_backbone(self):\n",
        "    \"\"\"\n",
        "    Unreeze the backbone trainable parameters. Use when fine-tuning.\n",
        "    \"\"\"\n",
        "    self.backbone.trainable = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "emzhyR-l21m4"
      },
      "outputs": [],
      "source": [
        "def yolo_loss_wrraper(anchors):\n",
        "  def yolo_loss(y_true, y_pred, lambda_coord=10, lambda_obj=1, lambda_no_obj=1e-1, lambda_cls=1):\n",
        "    \"\"\"\n",
        "    Classic yolo loss function. In this function we assume y_true correspond\n",
        "    to one scale only.\n",
        "\n",
        "    Args:\n",
        "    - y_true: A tensor of shape (batch_size, scale_grid_scale, scale_grid_scale, 3, 4 + 1 + 1)\n",
        "    - y_pred: A tensor of shape (batch_size, scale_grid_scale, scale_grid_scale, 3 * (4 + 1 + NUM_CLASSES))\n",
        "\n",
        "    Returns:\n",
        "    - loss value\n",
        "    \"\"\"\n",
        "    batch_size, grid_scale_x, grid_scale_y, _ = shape(y_pred)\n",
        "    y_pred = reshape(y_pred, newshape=(batch_size, grid_scale_x, grid_scale_y, 3, -1))\n",
        "\n",
        "    # coordinates loss\n",
        "    y_pred_xy = sigmoid(y_pred[..., 0:2])\n",
        "    y_pred_wh = exp(y_pred[..., 2:4]) * anchors\n",
        "    y_true_xy = y_true[..., 0:2]\n",
        "    y_true_wh = y_true[..., 2:4]\n",
        "\n",
        "    # object & no object masks\n",
        "    object_mask = y_true[..., 4:5]\n",
        "\n",
        "    grid_x = arange(0, grid_scale_x, dtype=\"float32\")\n",
        "    grid_y = arange(0, grid_scale_y, dtype=\"float32\")\n",
        "    grid_xs, grid_ys = meshgrid(grid_x, grid_y)\n",
        "    grid_xy = stack([grid_xs, grid_ys], axis=-1) # shape: (grid_scale_x, grid_scale_y, 2)\n",
        "    grid_xy = expand_dims(grid_xy, axis=-2) # shape: (grid_scale_x, grid_scale_y, 1, 2)\n",
        "\n",
        "    y_pred_xy_non_cell_rel = (y_pred_xy + grid_xy) / (grid_scale_x, grid_scale_y)\n",
        "    y_true_xy_non_cell_rel = (y_true_xy + grid_xy) / (grid_scale_x, grid_scale_y)\n",
        "\n",
        "    y_pred_xywh = concatenate([y_pred_xy_non_cell_rel, y_pred_wh], axis=-1)\n",
        "    y_true_xywh = concatenate([y_true_xy_non_cell_rel, y_true_wh], axis=-1)\n",
        "\n",
        "    iou = compute_iou(\n",
        "        reshape(y_pred_xywh, newshape=(-1, 4)),\n",
        "        reshape(y_true_xywh, newshape=(-1, 4)),\n",
        "        bounding_box_format=\"center_xywh\"\n",
        "        ) # shape: (batch_size * grid_scale_x * grid_scale_y * 3, ...)\n",
        "\n",
        "    iou = reshape(iou, newshape=(batch_size, grid_scale_x, grid_scale_y, 3, -1))\n",
        "    iou_max = max(iou, axis=-1, keepdims=True)\n",
        "\n",
        "    no_object_mask = where(iou_max < 0.6, 1 - object_mask, zeros_like(object_mask))\n",
        "\n",
        "    xy_loss = lambda_coord * sum(object_mask * (square(y_pred_xy - y_true_xy) + \\\n",
        "                                                square(sqrt(y_pred_wh) - sqrt(y_true_wh))))\n",
        "\n",
        "    # object confidence loss\n",
        "    y_pred_confidence = sigmoid(y_pred[..., 4:5])\n",
        "    y_true_confidence = y_true[..., 4:5]\n",
        "    bce_output = binary_crossentropy(y_true_confidence, y_pred_confidence) # shape: (batch_size, grid_scale_x, grid_scale_y, 3, 1)\n",
        "    confidence_loss = lambda_obj * sum(object_mask * bce_output) + \\\n",
        "                      lambda_no_obj * sum(no_object_mask * bce_output)\n",
        "\n",
        "    # classification loss\n",
        "    y_pred_cls = y_pred[..., 5:]\n",
        "    y_true_cls = y_true[..., 5]\n",
        "    sc_ce_output = expand_dims(sparse_categorical_crossentropy(y_true_cls, y_pred_cls, from_logits=True), axis=-1) # shape: (grid_scale_x, grid_scale_y, 3, 1)\n",
        "    classification_loss = lambda_cls * sum(object_mask * sc_ce_output)\n",
        "\n",
        "    return xy_loss + confidence_loss + classification_loss\n",
        "\n",
        "  return yolo_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRLS7rotEhQu"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gMRgo3yMIQY4"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.SGD(\n",
        "    learning_rate=1e-3,\n",
        "    momentum=0.9,\n",
        "    global_clipnorm=10.0\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2jrCwQXAEpcK"
      },
      "outputs": [],
      "source": [
        "loss_1 = yolo_loss_wrraper(np.array(ANCHORS[0], dtype=\"float64\") / IMAGE_DIMENSION)\n",
        "loss_2 = yolo_loss_wrraper(np.array(ANCHORS[1], dtype=\"float64\") / IMAGE_DIMENSION)\n",
        "loss_3 = yolo_loss_wrraper(np.array(ANCHORS[2], dtype=\"float64\") / IMAGE_DIMENSION)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [ReduceLROnPlateau(factor=0.2, patience=3, min_delta=1)]"
      ],
      "metadata": {
        "id": "GqAtgSEoxO8N"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_tToAedIS7g"
      },
      "outputs": [],
      "source": [
        "model = YoloSelfAttention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "MEaYen_4IXOx"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=[loss_1, loss_2, loss_3]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=eval_ds.take(250),\n",
        "    epochs=30,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "id": "coxHj35AcmUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b481026-dc5e-49f8-9bb6-3c563c5eb6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQtWXFkcJVVC"
      },
      "source": [
        "# Model Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St-3DBgXIrXF"
      },
      "outputs": [],
      "source": [
        "model.unfreeze_backbone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc6qEXqXP2Z_"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=1e-5,\n",
        "    global_clipnorm=10.0\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_1 = yolo_loss_wrraper(np.array(ANCHORS[0], dtype=\"float64\") / IMAGE_DIMENSION)\n",
        "loss_2 = yolo_loss_wrraper(np.array(ANCHORS[1], dtype=\"float64\") / IMAGE_DIMENSION)\n",
        "loss_3 = yolo_loss_wrraper(np.array(ANCHORS[2], dtype=\"float64\") / IMAGE_DIMENSION)"
      ],
      "metadata": {
        "id": "d72ETuGB1mDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [ReduceLROnPlateau(factor=0.2, patience=2, min_delta=1)]"
      ],
      "metadata": {
        "id": "UWh22pK3xjQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJKEAlyiP7e4"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=[loss_1, loss_2, loss_3]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=eval_ds.take(250),\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "id": "HPWuvN4Nck-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"yolov3_sa_voc2007.weights.h5\")"
      ],
      "metadata": {
        "id": "VdZVTmvOfzor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual tests"
      ],
      "metadata": {
        "id": "sJ6f5FE_b82I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, eval_ds = load(name=\"voc/2007\", split=[\"train\", 'validation'], with_info=False, shuffle_files=True)"
      ],
      "metadata": {
        "id": "vNQb_2QEb-4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_progress(model, confidence_threshold=0.5, iou_threshold=0.2, skip=500)"
      ],
      "metadata": {
        "id": "QRyvpv56b-8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanity Check"
      ],
      "metadata": {
        "id": "bpTTJ44_kVG-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLnKWc3Tpfix"
      },
      "outputs": [],
      "source": [
        "# Useful function for this checks section\n",
        "\n",
        "def scale_to_pred(scale, anchors):\n",
        "  grid_size, _, _, _ = shape(scale)\n",
        "  scale_xy = log(scale[..., 0:2] / (1- scale[..., 0:2]))\n",
        "  scale_wh = log(scale[..., 2:4] / anchors)\n",
        "  scale_confidence = where(scale[..., 4:5] == 1, 100, -100)\n",
        "  scale_cls = one_hot(cast(scale[..., 5], \"int32\"), NUM_CLASSES) * 200 - 100\n",
        "  scale_pred = concatenate([scale_xy, scale_wh, scale_confidence, scale_cls], axis=-1)\n",
        "  return reshape(scale_pred, newshape=(grid_size, grid_size, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbk9IG5yoLcT"
      },
      "outputs": [],
      "source": [
        "# Check if bbox_to_output_vec == bbox_to_output_loop & functions are correctly implemented\n",
        "\n",
        "inputs = {\"classes\": tf.constant([1, 2, 3]), \"boxes\": tf.constant([[0.35, 0.35, 0.2, 0.8], [0.38, 0.38, 0.3, 0.8], [0.24038462, 0.36057692, 0.12019231, 0.18028846]])}\n",
        "scale_1, scale_2, scale_3 = bbox_to_output_vec(**inputs)\n",
        "scale_1_l, scale_2_l, scale_3_l = bbox_to_output_loop(**inputs)\n",
        "tf.reduce_all(equal(scale_1, scale_1_l)), tf.reduce_all(equal(scale_2, scale_2_l)), tf.reduce_all(equal(scale_3, scale_3_l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z-Y8G9IpHcJ"
      },
      "outputs": [],
      "source": [
        "# Check the postproccessing step\n",
        "\n",
        "def post_proccessing(scale_1, scale_2, scale_3):\n",
        "  anchors_1 = np.array(ANCHORS[0])/IMAGE_DIMENSION\n",
        "  anchors_2 = np.array(ANCHORS[1])/IMAGE_DIMENSION\n",
        "  anchors_3 = np.array(ANCHORS[2])/IMAGE_DIMENSION\n",
        "\n",
        "  scale_1_pred = scale_to_pred(scale_1, anchors_1)\n",
        "  scale_2_pred = scale_to_pred(scale_2, anchors_2)\n",
        "  scale_3_pred = scale_to_pred(scale_3, anchors_3)\n",
        "\n",
        "  scale_1_reshaped = reshape(scale_1_pred, newshape=(1, GRID_SIZES[0], GRID_SIZES[0], -1))\n",
        "  scale_2_reshaped = reshape(scale_2_pred, newshape=(1, GRID_SIZES[1], GRID_SIZES[1], -1))\n",
        "  scale_3_reshaped = reshape(scale_3_pred, newshape=(1, GRID_SIZES[2], GRID_SIZES[2], -1))\n",
        "\n",
        "  box_prediction_1, class_prediction_1 = output_to_bbox(scale_1_reshaped, anchors=anchors_1)\n",
        "  box_prediction_2, class_prediction_2 = output_to_bbox(scale_2_reshaped, anchors=anchors_2)\n",
        "  box_prediction_3, class_prediction_3 = output_to_bbox(scale_3_reshaped, anchors=anchors_3)\n",
        "\n",
        "  box_prediction = concatenate([box_prediction_1, box_prediction_2, box_prediction_3], axis=1)\n",
        "  class_prediction = concatenate([class_prediction_1, class_prediction_2, class_prediction_3], axis=1)\n",
        "\n",
        "  nms = MultiClassNonMaxSuppression(\n",
        "      bounding_box_format=\"center_xywh\",\n",
        "      from_logits=False,\n",
        "      confidence_threshold=0.3\n",
        "      )\n",
        "\n",
        "  return nms(box_prediction, class_prediction)\n",
        "\n",
        "post_proccessing(scale_1, scale_2, scale_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzrIOEpibTDB"
      },
      "outputs": [],
      "source": [
        "def temp_preprocessing_wrapper(augmenters):\n",
        "  def temp_preprocessing(x):\n",
        "    \"\"\"\n",
        "    Tranforming a valid KerasCV format sample to be a proper input and label for model trainig.\n",
        "\n",
        "    Args:\n",
        "    - x: a valid KerasCV format.\n",
        "\n",
        "    Returns:\n",
        "    - image, labels: A tuple of the preprocessed image and (scale_1, scale_2, scale_3) label.\n",
        "    \"\"\"\n",
        "\n",
        "    # apply augmentations\n",
        "    for augmenter in augmenters:\n",
        "      x = augmenter(x)\n",
        "\n",
        "    image = x[\"images\"]\n",
        "\n",
        "    x[\"bounding_boxes\"] = to_dense(x[\"bounding_boxes\"])\n",
        "    x[\"bounding_boxes\"][\"boxes\"] = x[\"bounding_boxes\"][\"boxes\"] / IMAGE_DIMENSION\n",
        "    labels = bbox_to_output_vec(**x[\"bounding_boxes\"])\n",
        "    x[\"bounding_boxes\"][\"boxes\"] = x[\"bounding_boxes\"][\"boxes\"] * IMAGE_DIMENSION\n",
        "\n",
        "    return image, labels, x[\"bounding_boxes\"]\n",
        "\n",
        "  return temp_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MdErUVCGOib"
      },
      "outputs": [],
      "source": [
        "# Check visually the pipeline correctness\n",
        "\n",
        "sample = next(iter(train_ds.skip(0).take(1)))\n",
        "\n",
        "process_sample_fn = temp_preprocessing_wrapper(eval_ds_augmenters)\n",
        "image, (scale_1, scale_2, scale_3), bounding_boxes_true = process_sample_fn(sample)\n",
        "\n",
        "bounding_boxes_true = {\n",
        "    \"classes\": expand_dims(bounding_boxes_true[\"classes\"], axis=0),\n",
        "    \"boxes\": expand_dims(bounding_boxes_true[\"boxes\"], axis=0),\n",
        "    }\n",
        "\n",
        "bounding_boxes_pred = post_proccessing(scale_1, scale_2, scale_3)\n",
        "\n",
        "class_mapping = dict(zip(range(len(CLASSES)), CLASSES))\n",
        "plot_bounding_box_gallery(\n",
        "    expand_dims(image, axis=0),\n",
        "    value_range=(0, 255),\n",
        "    rows=1,\n",
        "    cols=1,\n",
        "    y_true=bounding_boxes_true,\n",
        "    y_pred=bounding_boxes_pred,\n",
        "    scale=5,\n",
        "    bounding_box_format=\"center_xywh\",\n",
        "    font_scale=0.6,\n",
        "    class_mapping=class_mapping\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOxJp2JUFK5DuzKiC/1jpWS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}